# One-Page GenMedia Eval Report (Template)

## Run metadata
- Date:
- Models compared:
- Clip length / resolution:
- Prompt set version:
- #samples (total / per task):

## Executive summary (3 bullets)
- Overall winner + win-rate:
- Biggest strengths observed:
- Biggest gaps / failures observed:

## Results overview
### Overall
- Win-rate (A vs B):
- Tie-rate:
- Avg rubric scores (adherence / temporal / identity / realism / edit precision):

### By task family
| Task | Win-rate | Key failure modes | Notes |
|------|----------|-------------------|------|
| T1 Adherence |  |  |  |
| T2 Temporal |  |  |  |
| T3 Identity |  |  |  |
| T4 Editability |  |  |  |
| T5 Relations/Timing |  |  |  |
| T6 Physics/Motion |  |  |  |
| T7 Safety |  |  |  |

## Failure mode breakdown (top 5)
1)  
2)  
3)  
4)  
5)  

## Example prompts (5 samples)
For each, include prompt + winner + reason tags.
1) Prompt:  
   Winner:  
   Tags:  
   Note:
2) â€¦

## Recommendations / Next iteration
- Benchmark updates (v0.2):
- Model-side experiments suggested:
- Data/rubric refinements:
